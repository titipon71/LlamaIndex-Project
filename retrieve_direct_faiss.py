# retrieve_direct_faiss.py
import os
import json
import numpy as np
from dotenv import load_dotenv

# ---------- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ----------
DATA_DIR = "data"
FAISS_DIR = "./faiss_db"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏î‡∏±‡∏ä‡∏ô‡∏µ (‡∏ñ‡πâ‡∏≤‡∏à‡∏∞ persist)
EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
TOP_K = int(os.getenv("TOP_K", 3))  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(FAISS_DIR, exist_ok=True)
load_dotenv()

# ---------- ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ + ‡∏ù‡∏±‡∏á‡∏•‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡πÇ‡∏ï‡∏£‡πå (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ) ----------
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter

# ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data/
documents = SimpleDirectoryReader(DATA_DIR).load_data()
if not documents:
    raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data/ ‡πÉ‡∏™‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á data/sample.txt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding model (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö LLM)
embed_model = HuggingFaceEmbedding(
    model_name=EMBED_MODEL,
    trust_remote_code=True
)

# ---------------------------------------------------------
# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ LlamaIndex + FAISSVectorStore
# ---------------------------------------------------------
import faiss
from llama_index.vector_stores.faiss import FaissVectorStore

# ‡∏´‡∏≤ dimension ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
_dummy_dim_vec = embed_model.get_text_embedding("dim_probe")
EMB_DIM = len(_dummy_dim_vec)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å metric: ‡πÉ‡∏ä‡πâ Inner Product (IP) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cosine similarity (‡∏Ñ‡∏ß‡∏£ normalize ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå)
faiss_index_for_li = faiss.IndexFlatIP(EMB_DIM)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store + storage context
faiss_store = FaissVectorStore(faiss_index=faiss_index_for_li)
storage_context = StorageContext.from_defaults(vector_store=faiss_store)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏î‡∏±‡∏ä‡∏ô‡∏µ (‡∏ù‡∏±‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ -> ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô FAISS)
# LlamaIndex ‡∏à‡∏∞‡∏î‡∏π‡πÅ‡∏•‡∏Å‡∏≤‡∏£ chunk ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏î‡∏µ‡∏ü‡∏≠‡∏•‡∏ï‡πå
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=embed_model,
    storage_context=storage_context,
)

# ---------------------------------------------------------
# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏Ñ‡∏¥‡∏ß‡∏£‡∏µ FAISS ‡∏ï‡∏£‡∏á ‡πÜ (‡∏ó‡∏≥‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏≠‡∏µ‡∏Å‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏ò‡∏¥‡∏ï)
# ---------------------------------------------------------
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ chunk ‡πÄ‡∏≠‡∏á + ‡∏ù‡∏±‡∏á‡πÄ‡∏≠‡∏á + build FAISS index ‡πÄ‡∏≠‡∏á
splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)
nodes = splitter.get_nodes_from_documents(documents)

texts = [n.get_content() for n in nodes]
metas = [n.metadata for n in nodes]

# ‡∏ù‡∏±‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡∏ä‡∏∏‡∏î (‡∏à‡∏∞‡πÉ‡∏ä‡πâ batch ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ‡πÅ‡∏ö‡∏ö‡∏ß‡∏ô loop)
emb_list = [embed_model.get_text_embedding(t) for t in texts]
emb = np.array(emb_list, dtype="float32")
# normalize L2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ IP ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡πà‡∏≤ cosine similarity
emb_norm = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)

# FAISS index (IP)
faiss_index_direct = faiss.IndexFlatIP(EMB_DIM)
faiss_index_direct.add(emb_norm)

# mapping id -> (text, meta) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
id2payload = [
    {
        "text": texts[i],
        "meta": metas[i]
    }
    for i in range(len(texts))
]

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡∏ó‡∏≥ persistence)
# faiss.write_index(faiss_index_direct, os.path.join(FAISS_DIR, "index_ip.faiss"))
# with open(os.path.join(FAISS_DIR, "payload.json"), "w", encoding="utf-8") as f:
#     json.dump(id2payload, f, ensure_ascii=False)

# ---------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏ú‡∏• ----------
def retrieve_with_llamaindex(query: str, top_k: int = TOP_K):
    retriever = index.as_retriever(similarity_top_k=top_k)
    nodes = retriever.retrieve(query)
    results = []
    for rank, n in enumerate(nodes, start=1):
        results.append({
            "rank": rank,
            "score": float(n.score),  # ‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ IP/cosine sim)
            "node_id": getattr(n, "node_id", getattr(n, "id_", None)),
            "source": n.metadata.get("file_path") or n.metadata.get("filename") or n.metadata,
            "text": n.get_content().strip()[:500]
        })
    return results

def retrieve_with_faiss(query: str, top_k: int = TOP_K):
    # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‚Üí normalize ‚Üí ‡∏Ñ‡∏¥‡∏ß‡∏£‡∏µ FAISS index
    q = np.array(embed_model.get_text_embedding(query), dtype="float32")[None, :]
    q = q / (np.linalg.norm(q, axis=1, keepdims=True) + 1e-12)
    sims, idxs = faiss_index_direct.search(q, top_k)  # sims = cosine similarity (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤ normalize ‡πÅ‡∏•‡πâ‡∏ß + ‡πÉ‡∏ä‡πâ IP)
    sims = sims[0]
    idxs = idxs[0]

    results = []
    for i, (score, idx) in enumerate(zip(sims, idxs), start=1):
        if idx == -1:
            continue
        payload = id2payload[idx]
        meta = payload["meta"]
        text = payload["text"]
        results.append({
            "rank": i,
            "score": float(score),  # ‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ (cosine sim)
            "source": (meta.get("file_path") or meta.get("filename") or meta),
            "text": (text or "")[:500],
        })
    return results

# ---------- CLI ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î ----------
def pretty_print(title: str, rows: list[dict]):
    print("\n" + "="*8 + f" {title} " + "="*8)
    if not rows:
        print("(no results)")
        return
    for r in rows:
        head = f"[{r['rank']}] score={r['score']:.4f} ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ)"
        print(head)
        print(f"source: {r['source']}")
        print(f"text  : {r['text']}\n")

if __name__ == "__main__":
    print("‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡πÇ‡∏ï‡∏£‡πå‡πÇ‡∏î‡∏¢ '‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ LLM' (FAISS)")
    print("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏ú‡∏•‡∏à‡∏≤‡∏Å 2 ‡∏ß‡∏¥‡∏ò‡∏µ (LlamaIndex Retriever / FAISS Direct)")
    print("‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å\n")
    while True:
        try:
            q = input("‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏î‡∏µ: ").strip()
            if not q:
                continue

            li = retrieve_with_llamaindex(q, TOP_K)
            pretty_print("LlamaIndex Retriever (FAISS, no LLM)", li)

            fd = retrieve_with_faiss(q, TOP_K)
            pretty_print("FAISS Direct Query (cosine sim via IP)", fd)
        except KeyboardInterrupt:
            print("\n‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö üëã")
            break
